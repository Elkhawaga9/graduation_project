# Graduation Project ‚Äì Progress Log

This file tracks the **technical progress** of my graduation project.
It is updated after completing each major step so anyone can clearly see
what is done and what is planned next.

---

## Project Overview

**Title:** RAG-based Question Answering for Computer Systems  
**Approach:** Retrieval-Augmented Generation (RAG)  
**LLM (planned):** Qwen 7B  
**Domain:** Computer Systems (textbooks, PDFs)

---

## Current Status

**Stage:** Data preprocessing & chunking  
**Status:** ‚úÖ Qwen 7B LLM Inference (Without RAG)

Last update: **Dec 24, 2025**

---

## Folder Structure

### `data_raw/`
Original raw data.
- `Computer Systems A Programmers Perspective by Randal E. Bryant, David R. OHallaron (z-lib.org).pdf` ‚Äì Source PDF.  
- `data_khawaga_4_10.csv` ‚Äì Additional dataset.

### `data_json/`
JSON representations of text chunks from PDFs.
- `*_chunks.json` ‚Äì Raw text chunks extracted from PDFs.

### `embeddings/`
Vector embeddings for enriched chunks.
- `*_chunks_enriched_embeddings.npy` ‚Äì Embeddings generated from enriched JSON chunks.

### `graduation_project/`
Main project folder for scripts, models, or core project code.

### `gradproj_env/`
Virtual environment folder (Python dependencies, configurations).

### `json_llm_responses/`
Structured LLM outputs from chunk enrichment.
- `*_chunks_enriched.json` ‚Äì Enriched JSON with cleaned text, topics, and subtopics.

### `raw_llm_responses/`
Raw LLM outputs before parsing.
- `*_chunks_raw.json` ‚Äì Direct LLM responses.

### `vector_store/`
Stores vector databases for retrieval (e.g., FAISS, Milvus, Chroma).

### `notebooks/`
Jupyter notebooks for preprocessing, embedding, and analysis.
- `01_preprocessing.ipynb` ‚Äì Initial data cleaning.  
- `01_single_pdf_processing.ipynb` ‚Äì Chunking PDFs into JSON.  
- `02_llm_deepseekr1t2_chunk_enrichment.ipynb` ‚Äì LLM-assisted enrichment.  
- `03_generate_embeddings.ipynb` ‚Äì Create embeddings from enriched JSON.  
- `04_store_embeddings.ipynb` ‚Äì Save embeddings for vector storage.

---


## Completed Steps

### ‚úÖ Step 1 ‚Äî Project Setup
- GitHub repository initialized
- Feature-branch workflow adopted
- Python virtual environment created
- Project folders organized

---

### ‚úÖ Step 2 ‚Äî Single PDF Processing

**Notebook:**  
`notebooks/01_single_pdf_processing.ipynb`

**Description:**
- Lists available PDFs from `data_raw/`
- User selects a PDF interactively
- Loads PDF using `PyPDFLoader`
- Combines all pages into one text
- Applies **paragraph-aware recursive chunking**
- Saves all chunks into **one JSON file per PDF**

**Output Format (JSON):**
```json
{
  "id": 1,
  "source": "example.pdf",
  "text": "chunk text here",
  "topic": "",
  "subtopic": ""
}
```

Design Notes:

Recursive chunking preferred over fixed splits

Paragraph boundaries prioritized

Generated data is not committed to GitHub

Planned Steps

---

### ‚úÖ Step 3 ‚Äî LLM-Assisted Chunk Enrichment

**Notebook:**  
`notebooks/02_llm_deepseekr1t2_chunk_enrichment.ipynb`

**Goal:**  
Enhance paragraph-aware chunks using DeepSeek R1T2 Chimera for RAG preprocessing, improving retrieval quality before embeddings.

**Input:**
- JSON chunks from Step 2
- Each chunk contains raw paragraph-level text

**Processing:**
- Send chunks to DeepSeek R1T2 Chimera (free via OpenRouter) using structured prompts
- Extract and/or generate:
  - Cleaned and normalized text
  - High-level topic labels (e.g., Memory, CPU, Cache, OS)
  - Subtopics (e.g., Virtual Memory, Pipelining, Cache Coherence)
- Validate and normalize LLM output into a consistent JSON schema

**Outputs:**
- **Raw LLM responses:** `raw_llm_responses/<pdf_name>_raw.json`  
  - Stores full LLM outputs per chunk
- **Parsed/enriched JSON:** `json_llm_responses/<pdf_name>_chunks_enriched.json`  
  - Structured JSON with cleaned text, topic, subtopic

**Example Output JSON:**
```json
{
  "id": 1,
  "source": "example.pdf",
  "original_text": "chunk text here",
  "llm_response": "cleaned text + topic/subtopic"
}
```
---
## ‚úÖ Step 4 ‚Äî Embeddings (Completed)

**Goal:**  
Convert enriched JSON chunks into embeddings for RAG retrieval.

**Input:**  
- JSON chunks from `json_llm_responses/`

**Processing:**  
- Select which JSON file to embed
- Generate embeddings using a sentensetransformer
- Save embeddings locally in `embeddings/` folder
- Each JSON file can have a separate embedding file

**Output:**  
- `.pkl` or `.parquet` files in `grad_embeddings/`
- Embeddings ready for vector storage in FAISS or Chroma

**Notes:**  
- PDFs and JSON files remain untracked in Git
- Supports free-tier embedding models to minimize cost

---
### ‚úÖ Step 5 ‚Äî Vector Store (Updated)

**Goal:** Store embeddings in a vector database for efficient retrieval.

**Implementation:**  
- Using **Chroma** as the vector store (persistent).  
- All enriched chunks from PDFs are embedded and stored in Chroma.  
- Metadata (`source`, `topic`, `subtopic`) has been normalized to primitive types (strings) for Chroma compatibility.  
- Supports **incremental updates** ‚Äî new PDFs can be added safely without overwriting existing embeddings.  
- Retrieval tested and ready for the RAG pipeline.

**Next Actions:**  
- Integrate with RAG pipeline to retrieve top-k chunks per query.  
- Use stored embeddings with **Qwen** or other LLMs to generate grounded answers.  
- Optionally add metadata filtering to improve retrieval relevance.

---

## ‚è≥ Step 6 ‚Äî Qwen 7B LLM Inference (Without RAG)

**Goal:**  
Establish a baseline question-answering system using Qwen 7B without external knowledge augmentation.

**Current Status:**  
- ‚úÖ Qwen 7B model successfully launched locally  
- ‚úÖ Inference notebook implemented and tested  
- ‚úÖ Model generates answers using only internal knowledge

**Completed Work:**  
- Loaded Qwen 7B model and tokenizer  
- Implemented prompt-based question answering  
- Evaluated responses on textbook-style questions  
- Recorded baseline performance (accuracy, verbosity, hallucination tendency)

**Outcome:**  
- Working standalone LLM pipeline  
- Baseline results for later comparison with RAG-enhanced inference

---

## ‚è≥ Step 7 ‚Äî Qwen 7B with Retrieval-Augmented Generation (RAG)

**Goal:**  
Enhance Qwen 7B responses by grounding answers in retrieved textbook context.

**Current Status:**  
- ‚úÖ Chroma vector store and retriever ready  
- üîÑ RAG integration in progress

**Planned Actions:**  
- Retrieve top-k relevant chunks from Chroma  
- Inject retrieved context into Qwen prompt  
- Design RAG prompt template (system + context + question)  
- Test RAG pipeline using the same questions as Step 6

**Evaluation Plan:**  
- Compare answers with and without RAG  
- Measure factual accuracy and relevance  
- Analyze hallucination reduction  
- Tune context length and chunk size

**Expected Outcome:**  
- Fully functional RAG pipeline using Qwen 7B  
- Demonstrated improvement over standalone LLM inference


# ‚è≥ Step 7 ‚Äî Evaluation

Test with Computer Systems questions

Analyze retrieval and answer quality

Notes

PDFs and generated JSON files are intentionally excluded from version control

Each notebook represents a single, clear pipeline stage

This file will be updated continuously throughout the project


---

### ‚úÖ How to add it to GitHub

```bash
git checkout -b docs_progress
git add PROGRESS.md
git commit -m "Add project progress tracking markdown file"
git push origin docs_progress
```

Then open a PR and merge.
