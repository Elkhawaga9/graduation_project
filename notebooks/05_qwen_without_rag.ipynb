{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeUdbgIN5-i5",
        "outputId": "83e4e64a-498c-49a1-d40a-8603a2e1ea26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q torch transformers accelerate bitsandbytes sentence-transformers rank_bm25"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import json\n",
        "import numpy as np\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "# 1. Setup the GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"System ready! Using device: {device}\")\n",
        "\n",
        "# 2. Load the small 'Similarity' Model (for checking text overlap)\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# 3. Load the big 'Grading' Model (Qwen 2.5 7B)\n",
        "# We load it in '4-bit' mode so it fits in the free Colab GPU\n",
        "model_id = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "print(\"Loading Qwen model... this may take 1-2 minutes...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "print(\"Qwen model loaded successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWPPJ-LQ9Owu",
        "outputId": "27d99ebc-cba9-4bbc-9a27-d9b855e4e4f7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "System ready! Using device: cuda\n",
            "Loading Qwen model... this may take 1-2 minutes...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8bfec19875cf4e0ca0763ea44c6dc7e0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2e2d0e5f26904ee69c43de12688ec423"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7583fb475eff4619afb700d21b20f7be"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2b8db67b83d049ccb7f48ace4c273190"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9635b834cf354ca4973c1739f7ec6ad2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f524eb17861d448fb499be40a8f9916c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6eb24bfe9aa341c7b342fd38ed3fa220"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00003-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f54aa9b9944941d1a7bfa78e42ec6c95"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1747dd1093ec43b7b907ea34241e6cc1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00004-of-00004.safetensors:   0%|          | 0.00/3.56G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "695e40c58aef426fa991d317fad1cdcf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "62dae2e16ce74918b2a240a032819645"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "34c4cf95272248c59fa7dc8cdf945e8d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ccbbe86e7dff44c284b318f743ca4cea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Qwen model loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "# --- PART A: Hybrid Similarity Logic ---\n",
        "def calculate_hybrid_similarity(student_ans, model_ans):\n",
        "    # 1. Semantic Score\n",
        "    embeddings = embedder.encode([student_ans, model_ans], convert_to_tensor=True)\n",
        "    semantic_score = util.cos_sim(embeddings[0], embeddings[1]).item()\n",
        "\n",
        "    # 2. Lexical Score (Jaccard)\n",
        "    set_student = set(student_ans.lower().split())\n",
        "    set_model = set(model_ans.lower().split())\n",
        "    intersection = len(set_student.intersection(set_model))\n",
        "    union = len(set_student.union(set_model))\n",
        "    lexical_score = intersection / union if union > 0 else 0\n",
        "\n",
        "    # 3. Combine\n",
        "    hybrid_score = (0.9 * semantic_score) + (0.1 * lexical_score)\n",
        "    return round(hybrid_score, 3)\n",
        "\n",
        "# --- PART B: The Corrected Grading Function ---\n",
        "def grade_student(question, student_ans, model_ans):\n",
        "    sim_score = calculate_hybrid_similarity(student_ans, model_ans)\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    You are a strict academic grading assistant. Grade the Student Answer based on the Model Answer and the Similarity Score.\n",
        "\n",
        "    ### INPUT DATA:\n",
        "    - **Question**: {question}\n",
        "    - **Model Answer**: {model_ans}\n",
        "    - **Student Answer**: {student_ans}\n",
        "    - **Hybrid Similarity Score**: {sim_score} (Range: 0.0 to 1.0)\n",
        "\n",
        "    ### GRADING RUBRIC:\n",
        "    - **1.0**: Perfect. Conceptually identical.\n",
        "    - **0.75**: Good. Correct concept but minor errors or weak terms.\n",
        "    - **0.5**: Partial. Main idea present but significant gaps.\n",
        "    - **0.25**: Poor. Mostly wrong, but one relevant keyword.\n",
        "    - **0.0**: Wrong.\n",
        "\n",
        "    ### REQUIREMENTS:\n",
        "    1. **Feedback**: 2 short sentences max.\n",
        "    2. **Format**: Output ONLY valid JSON.\n",
        "\n",
        "    ### OUTPUT JSON:\n",
        "    {{\n",
        "        \"grade\": <number>,\n",
        "        \"feedback\": \"<text>\"\n",
        "    }}\n",
        "    \"\"\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a grading machine. Output only JSON.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "\n",
        "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
        "\n",
        "    generated_ids = model.generate(\n",
        "        model_inputs.input_ids,\n",
        "        max_new_tokens=150,\n",
        "        temperature=0.05,\n",
        "        do_sample=True\n",
        "    )\n",
        "\n",
        "    # --- FIX START: Extract only the new tokens ---\n",
        "    # We slice the output to remove the input prompt\n",
        "    generated_ids = [\n",
        "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "    ]\n",
        "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "    # --- FIX END ---\n",
        "\n",
        "    # Robust JSON Cleaning\n",
        "    try:\n",
        "        # 1. Try to find the JSON object using regex (most reliable)\n",
        "        # Looks for content starting with { and ending with }\n",
        "        json_match = re.search(r\"\\{.*\\}\", response, re.DOTALL)\n",
        "\n",
        "        if json_match:\n",
        "            clean_json = json_match.group(0)\n",
        "            return json.loads(clean_json)\n",
        "        else:\n",
        "            return {\"error\": \"No JSON found in response\", \"raw_output\": response}\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\"error\": f\"JSON Parsing Failed: {str(e)}\", \"raw_output\": response}"
      ],
      "metadata": {
        "id": "VFkuPCya9RGW"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1: High Similarity\n",
        "q = \"What is the function of the CPU?\"\n",
        "m = \"The CPU executes instructions and processes data.\"\n",
        "s = \"The CPU is responsible for executing instructions and processing data.\"\n",
        "\n",
        "print(\"Grading Example 1...\")\n",
        "result = grade_student(q, s, m)\n",
        "print(json.dumps(result, indent=2))\n",
        "\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Example 2: Low Similarity but Correct (Qwen decides)\n",
        "q = \"Explain Python lists.\"\n",
        "m = \"A list is a mutable, ordered sequence of elements.\"\n",
        "s = \"It's like an array that can change size and hold different stuff.\"\n",
        "\n",
        "print(\"Grading Example 2...\")\n",
        "result = grade_student(q, s, m)\n",
        "print(json.dumps(result, indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neV3A0cf9bxW",
        "outputId": "650a06ce-7082-44bf-f19f-b51c4be988ed"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grading Example 1...\n",
            "{\n",
            "  \"grade\": 0.889,\n",
            "  \"feedback\": \"Conceptually correct and very similar to the model answer.\"\n",
            "}\n",
            "------------------------------\n",
            "Grading Example 2...\n",
            "{\n",
            "  \"grade\": 0.5,\n",
            "  \"feedback\": \"The student correctly identified that the list can change size and hold different types of data, but missed the key term 'mutable' and the concept of being an ordered sequence.\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MNmZ56k8_sF3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}