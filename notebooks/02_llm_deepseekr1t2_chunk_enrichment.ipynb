{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a63572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in e:\\graduation_project\\gradproj_env\\lib\\site-packages (1.2.0)\n",
      "Requirement already satisfied: langchain-community in e:\\graduation_project\\gradproj_env\\lib\\site-packages (0.4.1)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.2.1 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from langchain) (1.2.2)\n",
      "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from langchain) (1.0.5)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from langchain) (2.12.5)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (0.5.0)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (25.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (4.15.0)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (0.12.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.1->langchain) (3.0.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n",
      "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.3.0)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.1)\n",
      "Requirement already satisfied: httpx>=0.25.2 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (3.11.5)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain) (0.25.0)\n",
      "Requirement already satisfied: anyio in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (4.12.0)\n",
      "Requirement already satisfied: certifi in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (1.0.9)\n",
      "Requirement already satisfied: idna in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
      "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from langchain-community) (1.0.0)\n",
      "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from langchain-community) (2.0.45)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from langchain-community) (3.13.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from langchain-community) (2.12.0)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from langchain-community) (0.4.3)\n",
      "Requirement already satisfied: numpy>=1.26.2 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from langchain-community) (2.3.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.0.0 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain) (2.6.2)\n",
      "Requirement already satisfied: greenlet>=1 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.3.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (only if not already installed)\n",
    "# %pip install --upgrade google-generativeai tqdm python-dotenv\n",
    "#%pip install -q langchain langchain-google-genai google-generativeai\n",
    "#%pip install langchain.prompts\n",
    "#%pip install langchain langchain-community\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13a3f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"tngtech/deepseek-r1t2-chimera:free\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44b29e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\graduation_project\\gradproj_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\user1\\AppData\\Local\\Temp\\ipykernel_13524\\4028791353.py:6: FutureWarning: \n",
      "\n",
      "All support for the `google.generativeai` package has ended. It will no longer be receiving \n",
      "updates or bug fixes. Please switch to the `google.genai` package as soon as possible.\n",
      "See README for more details:\n",
      "\n",
      "https://github.com/google-gemini/deprecated-generative-ai-python/blob/main/README.md\n",
      "\n",
      "  import google.generativeai as genai\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.messages import HumanMessage, AIMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4aff241f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Load environment variables for API key\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "# Folder containing all JSON chunks from Step 1/2\n",
    "INPUT_DIR = Path(\"E:/graduation_project/data_json\")  # no filename, folder only\n",
    "\n",
    "# -----------------------------\n",
    "# Define folders\n",
    "# -----------------------------\n",
    "INPUT_DIR = Path(\"../data_json\")  # input JSON chunks\n",
    "RAW_RESPONSES_DIR = Path(\"../raw_llm_responses\")\n",
    "JSON_RESPONSES_DIR = Path(\"../json_llm_responses\")\n",
    "\n",
    "RAW_RESPONSES_DIR.mkdir(exist_ok=True)\n",
    "JSON_RESPONSES_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebefbd69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 JSON files:\n",
      "1. Computer Systems A Programmers Perspective by Randal E. Bryant, David R. OHallaron (z-lib.org)_chunks.json\n",
      "Selected file: Computer Systems A Programmers Perspective by Randal E. Bryant, David R. OHallaron (z-lib.org)_chunks.json\n"
     ]
    }
   ],
   "source": [
    "# List all JSON files in the input folder\n",
    "json_files = list(INPUT_DIR.glob(\"*.json\"))\n",
    "\n",
    "print(f\"Found {len(json_files)} JSON files:\")\n",
    "for i, f in enumerate(json_files):\n",
    "    print(f\"{i+1}. {f.name}\")\n",
    "\n",
    "# Select one JSON file\n",
    "file_index = int(input(\"Enter the number of the JSON file to process: \")) - 1\n",
    "selected_file = json_files[file_index]\n",
    "\n",
    "print(f\"Selected file: {selected_file.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17976e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3315 chunks\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(selected_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    chunks = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(chunks)} chunks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82dd39e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: models/embedding-gecko-001\n",
      "Name: models/gemini-2.5-flash\n",
      "Name: models/gemini-2.5-pro\n",
      "Name: models/gemini-2.0-flash-exp\n",
      "Name: models/gemini-2.0-flash\n",
      "Name: models/gemini-2.0-flash-001\n",
      "Name: models/gemini-2.0-flash-lite-001\n",
      "Name: models/gemini-2.0-flash-lite\n",
      "Name: models/gemini-2.0-flash-lite-preview-02-05\n",
      "Name: models/gemini-2.0-flash-lite-preview\n",
      "Name: models/gemini-exp-1206\n",
      "Name: models/gemini-2.5-flash-preview-tts\n",
      "Name: models/gemini-2.5-pro-preview-tts\n",
      "Name: models/gemma-3-1b-it\n",
      "Name: models/gemma-3-4b-it\n",
      "Name: models/gemma-3-12b-it\n",
      "Name: models/gemma-3-27b-it\n",
      "Name: models/gemma-3n-e4b-it\n",
      "Name: models/gemma-3n-e2b-it\n",
      "Name: models/gemini-flash-latest\n",
      "Name: models/gemini-flash-lite-latest\n",
      "Name: models/gemini-pro-latest\n",
      "Name: models/gemini-2.5-flash-lite\n",
      "Name: models/gemini-2.5-flash-image-preview\n",
      "Name: models/gemini-2.5-flash-image\n",
      "Name: models/gemini-2.5-flash-preview-09-2025\n",
      "Name: models/gemini-2.5-flash-lite-preview-09-2025\n",
      "Name: models/gemini-3-pro-preview\n",
      "Name: models/gemini-3-flash-preview\n",
      "Name: models/gemini-3-pro-image-preview\n",
      "Name: models/nano-banana-pro-preview\n",
      "Name: models/gemini-robotics-er-1.5-preview\n",
      "Name: models/gemini-2.5-computer-use-preview-10-2025\n",
      "Name: models/deep-research-pro-preview-12-2025\n",
      "Name: models/embedding-001\n",
      "Name: models/text-embedding-004\n",
      "Name: models/gemini-embedding-exp-03-07\n",
      "Name: models/gemini-embedding-exp\n",
      "Name: models/gemini-embedding-001\n",
      "Name: models/aqa\n",
      "Name: models/imagen-4.0-generate-preview-06-06\n",
      "Name: models/imagen-4.0-ultra-generate-preview-06-06\n",
      "Name: models/imagen-4.0-generate-001\n",
      "Name: models/imagen-4.0-ultra-generate-001\n",
      "Name: models/imagen-4.0-fast-generate-001\n",
      "Name: models/veo-2.0-generate-001\n",
      "Name: models/veo-3.0-generate-001\n",
      "Name: models/veo-3.0-fast-generate-001\n",
      "Name: models/veo-3.1-generate-preview\n",
      "Name: models/veo-3.1-fast-generate-preview\n",
      "Name: models/gemini-2.5-flash-native-audio-latest\n",
      "Name: models/gemini-2.5-flash-native-audio-preview-09-2025\n",
      "Name: models/gemini-2.5-flash-native-audio-preview-12-2025\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Load API key from .env\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "# Configure the Gemini API\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "# Get generator object\n",
    "models_gen = genai.list_models()\n",
    "\n",
    "# Convert generator to list to inspect\n",
    "models_list = list(models_gen)\n",
    "\n",
    "# Print all available models\n",
    "for model in models_list:\n",
    "    print(f\"Name: {model.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6872db9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk 1 preview:\n",
      "\n",
      "THIRD EDITION \n",
      ". \n",
      "COMPUTER SYSTEMS \n",
      "BRYANT • O'HALLARON \n",
      "I\n",
      "\n",
      "C·oni.puter Systems \n",
      "A Programmer's Perspective \n",
      "THIRD EDITION \n",
      "\"Randal E. Bryant \n",
      "Carnegie Mellon University \n",
      "David R. O'Hallaron \n",
      "Carnegie Mellon University \n",
      "Pearson \n",
      "Boston Columbus Hoboken Indianapolis New York San Francisco \n",
      "Amsterdam Cape Town Dubai London Madrid Milan Munich Paris Montre'h.1 .Jbro:rito \n",
      "Delhi Mexico City Sao Paulo Sydney Hong Kong Seoul Singapore Taipei Tokyo\n",
      "\n",
      "Chunk 2 preview:\n",
      "\n",
      "\\ \n",
      "I \n",
      "Vice President and Editorial Director: Marcia J. Horton \n",
      "Executive Editor: Matt Goldstein \n",
      "Editorial Assistant: Kelsey Loanes \n",
      "VP of Marketing: Christy Lesko \n",
      "Director of Field Marketing: Tim Galligan \n",
      "Product Marketing Manager: Bram van Kempen \n",
      "Field Marketing Manager: Demetrius Hall \n",
      "Marketing Assistant: Jon Bryant \n",
      "Director of Product Management: Erin Gregg \n",
      "Team Lead Product Management: Scott Disanno \n",
      "Program Manager: Joanne Manning \n",
      "Procurement Manager: Mary Fischer \n",
      "Senior Specialist\n",
      "\n",
      "Chunk 3 preview:\n",
      "\n",
      "Windfall Software \n",
      "Composition: Windfall Software \n",
      "Printer/Binder: Courier Westford \n",
      "Cover Printer: Courier Westford \n",
      "Typeface: 10/12 Times 10, ITC Stone Sans \n",
      "Tue graph on the front cover is a \"memory mountain\" that shows the measured read throughput of an Intel Core i7 processor \n",
      "as a function of spatial and temporal locality. \n",
      "Copyright© 2016, 2011, and 2003 by Pearson Education, Inc. or its affiliates. All Rights Reserved. Printed in the United States \n",
      "of America. This publication is protect\n",
      "\n",
      "Chunk 4 preview:\n",
      "\n",
      "within the Pearson Education Global Rights & Permissions department, please visit www.pearsoned.com/permissions/. \n",
      "Many of the designations by manufacturers and seller to distinguish their products are claimed as trademarks. Where those \n",
      "designations appear in this book, and the publisher was aware of a trademark claim, the designations have been printed in \n",
      "initial caps or all caps. \n",
      "Tue author and publisher of this book have used their best efforts in preparing this book. These efforts include\n",
      "\n",
      "Chunk 5 preview:\n",
      "\n",
      "performance, or use of these programs. \n",
      "Pearson Education Ltd., London \n",
      "Pearson Education Singapore, Pte. Ltd \n",
      "Pearson Education Canada, Inc. \n",
      "Pearson Education-Japan \n",
      "Pearson Education Australia PTY, Limited \n",
      "Pearson Education North Asia, Ltd., Hong Kong \n",
      "Pearson Educacill de Mexico, S.A. de C.V. \n",
      "Pearson Education Malaysia, Pte. Ltd. \n",
      "Pearson Education, Inc., Upper Saddle River, New Jersey \n",
      "Library of Congress Cataloging-in-Publication Data \n",
      "Bryant, Randal E. Computer systems: a programmer's p\n",
      "\n",
      "Chunk 6 preview:\n",
      "\n",
      "David R. (David Richard) II. Title. \n",
      "QA76.5.B795 2016 OOS.3--dc23 2015000930 \n",
      "10 9 8 7 6 5 4 3 2 \n",
      "PEARSON \n",
      "wmv.pearsonhighered.com \n",
      "ISBN 10: 0-13-409266-X \n",
      "ISBN 13: 978-0-13-409266-9\n",
      "\n",
      "Chunk 7 preview:\n",
      "\n",
      ",. \n",
      "To the ~tutjenfs ~lid instructors of the 15-213 \n",
      "course at Carnegie Mellon University, for inspiring \n",
      "us to develop and refine the material' fo'r this book.\n",
      "\n",
      "-\n",
      "Mastering Engineering® \n",
      "For Computer Systems: A Programmer's Perspective, Third Edition \n",
      "Mastering is Pearson's proven online Tutorial Homework program, newly available with the third \n",
      "edition of Computer Systems: A Programmer's Perspective. The Mastering platform allows you to \n",
      "integrate dynamic homework-with many problems taken dire\n",
      "\n",
      "Chunk 8 preview:\n",
      "\n",
      "Contents \n",
      "t \n",
      "\" \n",
      ",. \n",
      "Preface xix \n",
      "About the Autho~s xxxv \n",
      "1 \n",
      "A Tour of Cqmputer ~ystell!;s 1 \n",
      "Information Is,Bits + Context ''.l 1.1 \n",
      "L2 \n",
      "L3 \n",
      "1.4 \n",
      "Programs Are Translated .by Other P,rograms into Different Forms 4 \n",
      "It Pays to Understand How_ Compilation Systems Work 6 \n",
      "1.5 \n",
      "Processors Read and-Interpret Instructions Stored in Memory 7 \n",
      "1.4.1 Hardware Organization of a System 8 \n",
      "1.4.2 Running the hello Program 10 \n",
      "Caches Matter 11 \n",
      "1.6 Storage Devices Form a Hierarchy. 14 \n",
      "1. 7 The Operating Syste\n",
      "\n",
      "Chunk 9 preview:\n",
      "\n",
      "1.10 Summary 27 \n",
      "Bibliographic Notes 28 \n",
      "Solutions to Practice Problems 28 \n",
      "Part I Program Structui:e and.Execution \n",
      "2 \n",
      "Represepting and Manipulating Information Jl \n",
      "2.1 Information Storage 34 \n",
      "2.1.1 Hexadecimal Notation 36 \n",
      "2.1.2 Data Sizes 39 \n",
      "vii\n",
      "\n",
      "Chunk 10 preview:\n",
      "\n",
      "viii Contents \n",
      "2.1.3 Addressing and Byte Ordering 42 \n",
      "2.1.4 Representing Strings 49 \n",
      "2.1.5 Representing Code 49 \n",
      "2.1.6 Introduction to Boolean Algebra 50 \n",
      "2.1.7 Bit-Level Operations in C 54 \n",
      "2.1.8 Logical Operations in C 56 \n",
      "2.1.9 Shift Operations in C 57 \n",
      "2.2 Integer Representations 59 \n",
      "2.2.1 Integral Data Types 60 \n",
      "2.2.2 Unsigned Encodings 62 \n",
      "2.2.3 1\\vo's-Complement Encodings, 64 \n",
      "2.2.4 Conversions between Signed and Unsigned 70 \n",
      "2.2.~ Signed versus Unsigned inC 74 \n",
      "2.2.6 Expanding\"the Bit Re\n"
     ]
    }
   ],
   "source": [
    "# Test with first 2 chunks only\n",
    "test_chunks = chunks[:10]\n",
    "\n",
    "for i, chunk in enumerate(test_chunks):\n",
    "    print(f\"\\nChunk {i+1} preview:\\n\")\n",
    "    print(chunk[\"text\"][:500])  # first 500 chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8ee6f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "You are an expert in computer systems textbooks and technical content.\n",
    "Improve the following text for a RAG pipeline.\n",
    "\n",
    "Instructions:\n",
    "1. Correct typos and formatting.\n",
    "2. Merge broken lines and paragraphs while keeping paragraphs logically separated.\n",
    "3. Preserve all technical content exactly — do not hallucinate.\n",
    "4. Generate a concise topic and subtopic.\n",
    "Text:\n",
    "\\\"\\\"\\\"{chunk_text}\\\"\\\"\\\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9a4f04c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openrouter in e:\\graduation_project\\gradproj_env\\lib\\site-packages (0.1.1)\n",
      "Requirement already satisfied: httpcore>=1.0.9 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from openrouter) (1.0.9)\n",
      "Requirement already satisfied: httpx>=0.28.1 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from openrouter) (0.28.1)\n",
      "Requirement already satisfied: pydantic>=2.11.2 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from openrouter) (2.12.5)\n",
      "Requirement already satisfied: certifi in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from httpcore>=1.0.9->openrouter) (2025.11.12)\n",
      "Requirement already satisfied: h11>=0.16 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from httpcore>=1.0.9->openrouter) (0.16.0)\n",
      "Requirement already satisfied: anyio in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from httpx>=0.28.1->openrouter) (4.12.0)\n",
      "Requirement already satisfied: idna in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from httpx>=0.28.1->openrouter) (3.11)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from pydantic>=2.11.2->openrouter) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from pydantic>=2.11.2->openrouter) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from pydantic>=2.11.2->openrouter) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from pydantic>=2.11.2->openrouter) (0.4.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting langchain-openai\n",
      "  Downloading langchain_openai-1.1.6-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.2.2 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from langchain-openai) (1.2.2)\n",
      "Collecting openai<3.0.0,>=1.109.1 (from langchain-openai)\n",
      "  Downloading openai-2.13.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from langchain-openai) (0.12.0)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.2->langchain-openai) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.2->langchain-openai) (0.5.0)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.2->langchain-openai) (25.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.2->langchain-openai) (2.12.5)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.2->langchain-openai) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.2->langchain-openai) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.2->langchain-openai) (4.15.0)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.2->langchain-openai) (0.12.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.2->langchain-openai) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.2->langchain-openai) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.2->langchain-openai) (3.11.5)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.2->langchain-openai) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.2->langchain-openai) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.2->langchain-openai) (0.25.0)\n",
      "Requirement already satisfied: anyio in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.2->langchain-openai) (4.12.0)\n",
      "Requirement already satisfied: certifi in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.2->langchain-openai) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.2->langchain-openai) (1.0.9)\n",
      "Requirement already satisfied: idna in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.2->langchain-openai) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.2->langchain-openai) (0.16.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
      "Collecting jiter<1,>=0.10.0 (from openai<3.0.0,>=1.109.1->langchain-openai)\n",
      "  Downloading jiter-0.12.0-cp311-cp311-win_amd64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: sniffio in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.2->langchain-openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.2->langchain-openai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.2->langchain-openai) (0.4.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2025.11.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.2->langchain-openai) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.2->langchain-openai) (2.6.2)\n",
      "Requirement already satisfied: colorama in e:\\graduation_project\\gradproj_env\\lib\\site-packages (from tqdm>4->openai<3.0.0,>=1.109.1->langchain-openai) (0.4.6)\n",
      "Downloading langchain_openai-1.1.6-py3-none-any.whl (84 kB)\n",
      "Downloading openai-2.13.0-py3-none-any.whl (1.1 MB)\n",
      "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.3/1.1 MB ? eta -:--:--\n",
      "   ----------------------------- ---------- 0.8/1.1 MB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.1/1.1 MB 2.7 MB/s  0:00:00\n",
      "Downloading jiter-0.12.0-cp311-cp311-win_amd64.whl (204 kB)\n",
      "Installing collected packages: jiter, openai, langchain-openai\n",
      "\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   ------------- -------------------------- 1/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [langchain-openai]\n",
      "   -------------------------- ------------- 2/3 [langchain-openai]\n",
      "   ---------------------------------------- 3/3 [langchain-openai]\n",
      "\n",
      "Successfully installed jiter-0.12.0 langchain-openai-1.1.6 openai-2.13.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openrouter\n",
    "%pip install langchain-openai\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0efe19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: tngtech/deepseek-r1t2-chimera:free\n",
      "------------------------------\n",
      "Decorators are functions that wrap another function to extend its behavior, applied using the `@decorator_name` syntax above a function definition. For example, `@my_decorator` applies `my_decorator(func)` to the function below, allowing you to modify/add functionality without altering the original function.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "\n",
    "# 2. Initialize the ChatOpenAI client for OpenRouter\n",
    "llm = ChatOpenAI(\n",
    "    model=MODEL_NAME,  # You can use any OpenRouter model string\n",
    "    openai_api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "    openai_api_base=\"https://openrouter.ai/api/v1\",\n",
    "    default_headers={\n",
    "        \"HTTP-Referer\": \"http://localhost:8888\", # Optional: your site URL for rankings\n",
    "        \"X-Title\": \"My Notebook App\",           # Optional: your site name\n",
    "    }\n",
    ")\n",
    "\n",
    "# 3. Create a simple prompt\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful coding assistant.\"),\n",
    "    HumanMessage(content=\"Explain how to use decorators in Python in two sentences.\")\n",
    "]\n",
    "\n",
    "# 4. Invoke the model\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(f\"Model: {response.response_metadata.get('model_name', 'Unknown')}\")\n",
    "print(\"-\" * 30)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4a79f380",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating LLM raw responses:   1%|▏         | 46/3315 [41:37<49:18:23, 54.30s/it]\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '50', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1766188800000'}, 'provider_name': None}}, 'user_id': 'user_340nYGGQR6AleRV9MLwPbHFw93r'}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m prompt_text = PROMPT_TEMPLATE.format(chunk_text=chunk[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# LangChain LLM call\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m response = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt_text\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m llm_text = response.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m].text  \u001b[38;5;66;03m# <-- this is already a string, safe to store\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Save only the text content, not full AIMessage object\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\graduation_project\\gradproj_env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:927\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    924\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    925\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    926\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m927\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    933\u001b[39m         )\n\u001b[32m    934\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    935\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\graduation_project\\gradproj_env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1221\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1219\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1220\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1221\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1225\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\graduation_project\\gradproj_env\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1380\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1378\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[33m\"\u001b[39m\u001b[33mhttp_response\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1379\u001b[39m         e.response = raw_response.http_response  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1380\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   1381\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1382\u001b[39m     \u001b[38;5;28mself\u001b[39m.include_response_headers\n\u001b[32m   1383\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1384\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1385\u001b[39m ):\n\u001b[32m   1386\u001b[39m     generation_info = {\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response.headers)}\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\graduation_project\\gradproj_env\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1375\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1368\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _construct_lc_result_from_responses_api(\n\u001b[32m   1369\u001b[39m             response,\n\u001b[32m   1370\u001b[39m             schema=original_schema_obj,\n\u001b[32m   1371\u001b[39m             metadata=generation_info,\n\u001b[32m   1372\u001b[39m             output_version=\u001b[38;5;28mself\u001b[39m.output_version,\n\u001b[32m   1373\u001b[39m         )\n\u001b[32m   1374\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1375\u001b[39m         raw_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_raw_response\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1376\u001b[39m         response = raw_response.parse()\n\u001b[32m   1377\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\graduation_project\\gradproj_env\\Lib\\site-packages\\openai\\_legacy_response.py:364\u001b[39m, in \u001b[36mto_raw_response_wrapper.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    360\u001b[39m extra_headers[RAW_RESPONSE_HEADER] = \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    362\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mextra_headers\u001b[39m\u001b[33m\"\u001b[39m] = extra_headers\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\graduation_project\\gradproj_env\\Lib\\site-packages\\openai\\_utils\\_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\graduation_project\\gradproj_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:1192\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1145\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1146\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1147\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1189\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   1190\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1191\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1199\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1200\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1201\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1202\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1203\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1204\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1205\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1206\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1207\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1208\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1209\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1210\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1211\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1212\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1213\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_retention\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_retention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1214\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1215\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1216\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1217\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1218\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1219\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1220\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1221\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1225\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1226\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1227\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1228\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1229\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1230\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1231\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1232\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1237\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1238\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1239\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1242\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\graduation_project\\gradproj_env\\Lib\\site-packages\\openai\\_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\graduation_project\\gradproj_env\\Lib\\site-packages\\openai\\_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '50', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1766188800000'}, 'provider_name': None}}, 'user_id': 'user_340nYGGQR6AleRV9MLwPbHFw93r'}"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Generate raw LLM responses\n",
    "# -----------------------------\n",
    "import json\n",
    "from urllib import response\n",
    "from tqdm import tqdm\n",
    "from langchain.messages import HumanMessage\n",
    "\n",
    "raw_responses = []\n",
    "\n",
    "for chunk in tqdm(chunks, desc=\"Generating LLM raw responses\"):\n",
    "    prompt_text = PROMPT_TEMPLATE.format(chunk_text=chunk[\"text\"])\n",
    "\n",
    "    # LangChain LLM call\n",
    "    response = llm.generate([[HumanMessage(content=prompt_text)]])\n",
    "    llm_text = response.generations[0][0].text  # <-- this is already a string, safe to store\n",
    "\n",
    "    # Save only the text content, not full AIMessage object\n",
    "    raw_responses.append({\n",
    "        \"id\": chunk[\"id\"],\n",
    "        \"source\": chunk[\"source\"],\n",
    "        \"original_text\": chunk[\"text\"],\n",
    "        \"llm_response\": llm_text\n",
    "    })\n",
    "\n",
    "# Save raw responses\n",
    "raw_output_file = RAW_RESPONSES_DIR / f\"{selected_file.stem}_raw.json\"\n",
    "with open(raw_output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(raw_responses, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✅ Raw LLM responses saved to {raw_output_file}\")\n",
    "\n",
    "# Create output folder if it doesn't exist\n",
    "RAW_RESPONSES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save raw responses\n",
    "raw_output_file = RAW_RESPONSES_DIR / f\"{selected_file.stem}_raw.json\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "390a78b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 1, 'source': 'Computer Systems A Programmers Perspective by Randal E. Bryant, David R. OHallaron (z-lib.org).pdf', 'original_text': 'THIRD EDITION \\n. \\nCOMPUTER SYSTEMS \\nBRYANT • O\\'HALLARON \\nI\\n\\nC·oni.puter Systems \\nA Programmer\\'s Perspective \\nTHIRD EDITION \\n\"Randal E. Bryant \\nCarnegie Mellon University \\nDavid R. O\\'Hallaron \\nCarnegie Mellon University \\nPearson \\nBoston Columbus Hoboken Indianapolis New York San Francisco \\nAmsterdam Cape Town Dubai London Madrid Milan Munich Paris Montre\\'h.1 .Jbro:rito \\nDelhi Mexico City Sao Paulo Sydney Hong Kong Seoul Singapore Taipei Tokyo', 'llm_response': '**Topic:** Computer Science Textbook - Computer Systems  \\n**Subtopic:** Third Edition Publication Details  \\n\\n---\\n\\n**Improved Text:**  \\nTHIRD EDITION  \\nCOMPUTER SYSTEMS  \\nA Programmer\\'s Perspective  \\n\\nRandal E. Bryant  \\nCarnegie Mellon University  \\n\\nDavid R. O\\'Hallaron  \\nCarnegie Mellon University  \\n\\nPearson  \\nBoston, Columbus, Hoboken, Indianapolis, New York, San Francisco, Amsterdam, Cape Town, Dubai, London, Madrid, Milan, Munich, Paris, Montréal, Toronto, Delhi, Mexico City, São Paulo, Sydney, Hong Kong, Seoul, Singapore, Taipei, Tokyo  \\n\\n---\\n\\n**Key Improvements:**  \\n1. Corrected typos (e.g., \"C·oni.puter\" → \"Computer\", \"Montre\\'h.1 .Jbro:rito\" → \"Montréal, Toronto\").  \\n2. Merged broken lines into logical paragraphs while preserving original content.  \\n3. Standardized spacing/punctuation in publisher locations.  \\n4. Retained all bibliographic details verbatim (authors, affiliations, publisher, cities).'}\n",
      "✅ Raw LLM responses saved to ..\\raw_llm_responses\\Computer Systems A Programmers Perspective by Randal E. Bryant, David R. OHallaron (z-lib.org)_chunks_raw.json\n"
     ]
    }
   ],
   "source": [
    "print(raw_responses[0])\n",
    "\n",
    "with open(raw_output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(raw_responses, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✅ Raw LLM responses saved to {raw_output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c1510a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing raw responses: 100%|██████████| 46/46 [00:00<00:00, 11529.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Enriched JSON saved to ..\\json_llm_responses\\Computer Systems A Programmers Perspective by Randal E. Bryant, David R. OHallaron (z-lib.org)_chunks_chunks_enriched.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Paths\n",
    "RAW_RESPONSES_DIR = Path(\"../raw_llm_responses\")\n",
    "JSON_RESPONSES_DIR = Path(\"../json_llm_responses\")\n",
    "JSON_RESPONSES_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Load raw responses\n",
    "raw_file = RAW_RESPONSES_DIR / f\"{selected_file.stem}_raw.json\"\n",
    "with open(raw_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_responses = json.load(f)\n",
    "\n",
    "enriched_chunks = []\n",
    "\n",
    "for chunk in tqdm(raw_responses, desc=\"Parsing raw responses\"):\n",
    "    llm_text = chunk[\"llm_response\"]\n",
    "\n",
    "    # Extract improved text\n",
    "    improved_text = \"\"\n",
    "    topic = \"\"\n",
    "    subtopic = []\n",
    "\n",
    "    try:\n",
    "        # Attempt to extract text inside ``` ```\n",
    "        if \"```\" in llm_text:\n",
    "            improved_text = llm_text.split(\"```\")[1].strip()\n",
    "        else:\n",
    "            improved_text = llm_text\n",
    "\n",
    "        # Attempt to extract topic\n",
    "        if \"**Topic:**\" in llm_text:\n",
    "            topic_line = llm_text.split(\"**Topic:**\")[1].split(\"\\n\")[0].strip()\n",
    "            topic = topic_line\n",
    "\n",
    "        # Attempt to extract subtopics (if any)\n",
    "        if \"**Subtopic:**\" in llm_text:\n",
    "            subtopic_line = llm_text.split(\"**Subtopic:**\")[1].split(\"\\n\")[0].strip()\n",
    "            subtopic = [s.strip() for s in subtopic_line.split(\" - \")]\n",
    "        elif \"**Subtopics:**\" in llm_text:\n",
    "            subtopic_section = llm_text.split(\"**Subtopics:**\")[1].split(\"\\n\\n\")[0]\n",
    "            subtopic = [line.strip(\"- \").strip() for line in subtopic_section.split(\"\\n\") if line.strip()]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Failed to parse chunk {chunk['id']}: {e}\")\n",
    "        improved_text = llm_text\n",
    "\n",
    "    enriched_chunks.append({\n",
    "        \"id\": chunk[\"id\"],\n",
    "        \"source\": chunk[\"source\"],\n",
    "        \"original_text\": chunk[\"original_text\"],\n",
    "        \"text\": improved_text,\n",
    "        \"topic\": topic,\n",
    "        \"subtopic\": subtopic\n",
    "    })\n",
    "\n",
    "# Save parsed JSON\n",
    "enriched_file = JSON_RESPONSES_DIR / f\"{selected_file.stem}_chunks_enriched.json\"\n",
    "with open(enriched_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(enriched_chunks, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✅ Enriched JSON saved to {enriched_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f84779",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gradproj_env (3.11.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
