{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["\n","â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","NOTEBOOK 01: DATA PREPARATION & REFERENCE PROCESSING\n","â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","CS Theory Auto-Grading System - Graduation Project\n","\n","Purpose:\n","  1. Setup Google Drive structure\n","  2. Process student data (train/val/test splits)\n","  3. Process reference materials (textbooks)\n","  4. Create vector stores for RAG\n","\n","Runtime: 30-45 minutes (first run)\n","Run: ONCE (saves everything to Google Drive)\n","\n","```text\n","MyDrive/project/\n","â”œâ”€â”€ data_processed/\n","â”‚   â”œâ”€â”€ train_df.pkl\n","â”‚   â”œâ”€â”€ val_df.pkl\n","â”‚   â””â”€â”€ test_df.pkl\n","â”œâ”€â”€ features/\n","â”‚   â”œâ”€â”€ student_vector_store.pkl\n","â”‚   â””â”€â”€ reference_vector_store.pkl\n","â””â”€â”€ config.json\n","\n","Author: [Elkhawaga - Shebita]\n","Date: 12 December 2025\n","â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","\n"],"metadata":{"id":"A2_ZavqoS2oB"}},{"cell_type":"code","source":["# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","# CELL 1: Mount Google Drive & Check GPU\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","\n","import pandas as pd\n","import numpy as np\n","import torch\n","import json\n","import warnings\n","from typing import List, Dict\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_absolute_error\n","import gc\n","import torch\n","from google.colab import drive\n","\n","print(\"ğŸ”— Mounting Google Drive...\")\n","drive.mount('/content/drive')\n","print(\"âœ… Google Drive mounted!\\n\")\n","\n","print(\"ğŸ” Checking GPU availability...\")\n","print(f\"GPU Available: {torch.cuda.is_available()}\")\n","\n","if torch.cuda.is_available():\n","    print(f\"âœ… GPU: {torch.cuda.get_device_name(0)}\")\n","    print(f\"ğŸ’¾ Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n","else:\n","    print(\"âš ï¸  WARNING: No GPU detected!\")\n","    print(\"   Go to: Runtime > Change runtime type > Select T4 GPU\")\n","    raise Exception(\"GPU required! Please enable GPU and restart.\")"],"metadata":{"id":"j_hJwYkMTgah","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765509123191,"user_tz":-120,"elapsed":4228,"user":{"displayName":"Ahmed Salah","userId":"18015139272935664527"}},"outputId":"fbb6b41b-728e-4231-e829-73d5ac28896a"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ”— Mounting Google Drive...\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","âœ… Google Drive mounted!\n","\n","ğŸ” Checking GPU availability...\n","GPU Available: True\n","âœ… GPU: Tesla T4\n","ğŸ’¾ Memory: 15.83 GB\n"]}]},{"cell_type":"code","source":["# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","# CELL 2: Setup Project Directory Structure\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","\n","import os\n","import json\n","\n","# Define project root in Google Drive\n","PROJECT_ROOT = '/content/drive/MyDrive/project'\n","\n","print(\"ğŸ“ Setting up project directory structure...\")\n","print(f\"   Root: {PROJECT_ROOT}\\n\")\n","\n","# Create directory structure\n","directories = {\n","    'data_raw': 'Raw data files',\n","    'data_processed': 'Processed train/val/test splits',\n","    'features': 'Vector stores and embeddings',\n","    'models': 'Model checkpoints and configs',\n","    'results': 'Grading results and metrics',\n","    'visualizations': 'Charts and graphs',\n","    'notebooks': 'Jupyter notebooks',\n","    'utils': 'Helper functions and utilities'\n","}\n","\n","for dir_name, description in directories.items():\n","    dir_path = os.path.join(PROJECT_ROOT, dir_name)\n","    os.makedirs(dir_path, exist_ok=True)\n","    print(f\"   âœ… {dir_name:<20} - {description}\")\n","\n","print(f\"\\nâœ… Project structure created at: {PROJECT_ROOT}\")\n","\n","# Create initial config\n","config = {\n","    'project_name': 'CS_Theory_Auto_Grading',\n","    'created_date': None,  # Will be set when processing\n","    'paths': {\n","        'project_root': PROJECT_ROOT,\n","        'data_raw': os.path.join(PROJECT_ROOT, 'data_raw'),\n","        'data_processed': os.path.join(PROJECT_ROOT, 'data_processed'),\n","        'features': os.path.join(PROJECT_ROOT, 'features'),\n","        'models': os.path.join(PROJECT_ROOT, 'models'),\n","        'results': os.path.join(PROJECT_ROOT, 'results'),\n","        'visualizations': os.path.join(PROJECT_ROOT, 'visualizations')\n","    },\n","    'data_info': {},\n","    'model_info': {\n","        'embedding_model': 'all-MiniLM-L6-v2',\n","        'grading_model': 'mistralai/Mistral-7B-Instruct-v0.3'\n","    }\n","}\n","\n","config_path = os.path.join(PROJECT_ROOT, 'config.json')\n","with open(config_path, 'w') as f:\n","    json.dump(config, f, indent=2)\n","\n","print(f\"âœ… Config saved: {config_path}\")"],"metadata":{"id":"vKaz-1hBTqgQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765509126102,"user_tz":-120,"elapsed":21,"user":{"displayName":"Ahmed Salah","userId":"18015139272935664527"}},"outputId":"b671eadc-2244-43c5-9386-74dc083b6d96"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ“ Setting up project directory structure...\n","   Root: /content/drive/MyDrive/project\n","\n","   âœ… data_raw             - Raw data files\n","   âœ… data_processed       - Processed train/val/test splits\n","   âœ… features             - Vector stores and embeddings\n","   âœ… models               - Model checkpoints and configs\n","   âœ… results              - Grading results and metrics\n","   âœ… visualizations       - Charts and graphs\n","   âœ… notebooks            - Jupyter notebooks\n","   âœ… utils                - Helper functions and utilities\n","\n","âœ… Project structure created at: /content/drive/MyDrive/project\n","âœ… Config saved: /content/drive/MyDrive/project/config.json\n"]}]},{"cell_type":"code","source":["# ============================================================================\n","# CELL 3: HuggingFace Authentication\n","# ============================================================================\n","from huggingface_hub import login\n","# Install required packages\n","print(\"\\nğŸ“¦ Installing packages...\")\n","!pip install -q transformers==4.36.0 accelerate bitsandbytes\n","!pip install -q langchain==0.1.0 langchain-community\n","!pip install -q sentence-transformers==2.2.2\n","!pip install -q chromadb==0.4.22\n","!pip install -q huggingface_hub\n","print(\"âœ… Installation complete!\")\n","# Get your token from: https://huggingface.co/settings/tokens\n","# You must accept the license at: https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct\n","HF_TOKEN = \"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\"  # Replace with your token  # Replace with your token\n","\n","login(token=HF_TOKEN)\n","print(\"âœ… Logged in to HuggingFace!\")\n"],"metadata":{"id":"Bi_gXk0dT1wR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765509157218,"user_tz":-120,"elapsed":28183,"user":{"displayName":"Ahmed Salah","userId":"18015139272935664527"}},"outputId":"80fed1ac-044d-41f4-8480-c8112d3cfbf5"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","ğŸ“¦ Installing packages...\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\n","jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n","opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n","pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n","opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n","xarray 2025.12.0 requires packaging>=24.1, but you have packaging 23.2 which is incompatible.\n","shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n","jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n","db-dtypes 1.4.4 requires packaging>=24.2.0, but you have packaging 23.2 which is incompatible.\n","opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mâœ… Installation complete!\n","âœ… Logged in to HuggingFace!\n"]}]},{"cell_type":"code","source":["# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","# CELL 5: Upload Student Data CSV\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","\n","from google.colab import files\n","\n","# Upload your CSV file (q, model_answer, student_answer, grade)\n","print(\"ğŸ“¤ Please upload your grading_data.csv file:\")\n","uploaded = files.upload()\n","\n"],"metadata":{"id":"QLKSX5VLUHTX","colab":{"base_uri":"https://localhost:8080/","height":92},"executionInfo":{"status":"ok","timestamp":1765509195848,"user_tz":-120,"elapsed":38618,"user":{"displayName":"Ahmed Salah","userId":"18015139272935664527"}},"outputId":"86374c0b-2f18-4931-bb6f-b46426088ecb"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ“¤ Please upload your grading_data.csv file:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-83a3750d-55c9-4ae7-b608-1354907c7b49\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-83a3750d-55c9-4ae7-b608-1354907c7b49\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving data_khawaga_4_10.csv to data_khawaga_4_10 (1).csv\n"]}]},{"cell_type":"code","source":["\n","# Get the filename\n","csv_filename = list(uploaded.keys())[0]\n","print(f\"âœ… File uploaded: {csv_filename}\")\n","\n","# Quick preview\n","df = pd.read_csv(csv_filename)\n","print(f\"\\nğŸ“Š Dataset shape: {df.shape}\")\n","print(f\"Columns: {list(df.columns)}\")\n","print(f\"\\nFirst few rows:\")\n","print(df.head())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uTfEM5fih3Ki","executionInfo":{"status":"ok","timestamp":1765509204043,"user_tz":-120,"elapsed":85,"user":{"displayName":"Ahmed Salah","userId":"18015139272935664527"}},"outputId":"d08a5c1f-a795-48ba-afe5-63add440c785"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… File uploaded: data_khawaga_4_10 (1).csv\n","\n","ğŸ“Š Dataset shape: (8352, 6)\n","Columns: ['id', 'question', 'model_answer', 'student_answer', 'grade', 'source']\n","\n","First few rows:\n","   id                                           question  \\\n","0   1  What is the role of a prototype program in pro...   \n","1   1  What is the role of a prototype program in pro...   \n","2   1  What is the role of a prototype program in pro...   \n","3   1  What is the role of a prototype program in pro...   \n","4   1  What is the role of a prototype program in pro...   \n","\n","                                        model_answer  \\\n","0  To simulate the behaviour of portions of the d...   \n","1  To simulate the behaviour of portions of the d...   \n","2  To simulate the behaviour of portions of the d...   \n","3  To simulate the behaviour of portions of the d...   \n","4  To simulate the behaviour of portions of the d...   \n","\n","                                      student_answer  grade  source  \n","0  High risk problems are address in the prototyp...    0.7  mohler  \n","1  To simulate portions of the desired final prod...    1.0  mohler  \n","2  A prototype program simulates the behaviors of...    0.8  mohler  \n","3  Defined in the Specification phase a prototype...    1.0  mohler  \n","4  It is used to let the users have a first idea ...    0.6  mohler  \n"]}]},{"cell_type":"code","source":["# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","# CELL 6: Upload Reference Books (PDFs)\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"ğŸ“š UPLOAD REFERENCE MATERIALS\")\n","print(\"=\"*70)\n","print(\"\\nPlease upload your reference textbooks (PDFs)\")\n","print(\"   1. Computer Systems: A Programmer's Perspective\")\n","print(\"   2. Any other CS textbooks/references\\n\")\n","print(\"âš ï¸  Note: Large PDFs may take a few minutes to upload\\n\")\n","\n","ref_uploaded = files.upload()\n","\n"],"metadata":{"id":"e_asmURQUJsf","colab":{"base_uri":"https://localhost:8080/","height":496},"executionInfo":{"status":"error","timestamp":1765509623862,"user_tz":-120,"elapsed":416355,"user":{"displayName":"Ahmed Salah","userId":"18015139272935664527"}},"outputId":"ca88d3f7-574c-45a0-f13c-af0ffcf20d6e"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======================================================================\n","ğŸ“š UPLOAD REFERENCE MATERIALS\n","======================================================================\n","\n","Please upload your reference textbooks (PDFs)\n","   1. Computer Systems: A Programmer's Perspective\n","   2. Any other CS textbooks/references\n","\n","âš ï¸  Note: Large PDFs may take a few minutes to upload\n","\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-274fd42b-c720-43b6-b9b6-1e4a681649c5\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-274fd42b-c720-43b6-b9b6-1e4a681649c5\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving Computer Systems A Programmers Perspective by Randal E. Bryant, David R. OHallaron (z-lib.org).pdf to Computer Systems A Programmers Perspective by Randal E. Bryant, David R. OHallaron (z-lib.org).pdf\n"]},{"output_type":"error","ename":"NameError","evalue":"name 'shutil' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2081586153.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0msource_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mdest_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPROJECT_ROOT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data_raw'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mreference_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdest_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"   âœ… Saved: {filename}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'shutil' is not defined"]}]},{"cell_type":"code","source":["import shutil\n","# Save PDFs to data_raw\n","reference_files = []\n","for filename in ref_uploaded.keys():\n","    source_path = filename\n","    dest_path = os.path.join(PROJECT_ROOT, 'data_raw', filename)\n","    shutil.move(source_path, dest_path)\n","    reference_files.append(dest_path)\n","    print(f\"   âœ… Saved: {filename}\")\n","\n","print(f\"\\nâœ… Uploaded {len(reference_files)} reference file(s)\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tbjb7BK0k15i","executionInfo":{"status":"ok","timestamp":1765509716713,"user_tz":-120,"elapsed":2603,"user":{"displayName":"Ahmed Salah","userId":"18015139272935664527"}},"outputId":"f5d93661-1e93-47cf-a119-18d772c2bbc9"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["   âœ… Saved: Computer Systems A Programmers Perspective by Randal E. Bryant, David R. OHallaron (z-lib.org).pdf\n","\n","âœ… Uploaded 1 reference file(s)\n"]}]},{"cell_type":"code","source":["# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","# CELL 7: Process Student Data (Train/Val/Test Split)\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","\n","def load_and_split_data(csv_path):\n","    \"\"\"Load and split student grading data\"\"\"\n","\n","    print(\"\\n\" + \"=\"*70)\n","    print(\"ğŸ“Š PROCESSING STUDENT DATA\")\n","    print(\"=\"*70)\n","\n","    # Use engine='python' for more robust parsing of malformed CSVs\n","    df = pd.read_csv(csv_path, encoding='latin1', on_bad_lines='skip', engine='python')\n","\n","    # Rename columns if needed\n","    if 'question' in df.columns and 'q' not in df.columns:\n","        df = df.rename(columns={'question': 'q'})\n","        print(\"âœ… Renamed 'question' â†’ 'q'\")\n","\n","    # Validate\n","    required_cols = ['q', 'model_answer', 'student_answer', 'grade']\n","    missing = [col for col in required_cols if col not in df.columns]\n","    if missing:\n","        raise ValueError(f\"Missing columns: {missing}\")\n","\n","    # Clean\n","    original_len = len(df)\n","    df = df.dropna(subset=required_cols)\n","    dropped = original_len - len(df)\n","\n","    if dropped > 0:\n","        print(f\"âš ï¸  Dropped {dropped} rows with missing values\")\n","\n","    df['student_answer'] = df['student_answer'].astype(str)\n","    df['model_answer'] = df['model_answer'].astype(str)\n","\n","    print(f\"\\nğŸ“ˆ Dataset Summary:\")\n","    print(f\"   Total samples:      {len(df):,}\")\n","    print(f\"   Unique questions:   {df['q'].nunique()}\")\n","    print(f\"   Grade range:        {df['grade'].min():.2f} - {df['grade'].max():.2f}\")\n","    print(f\"   Mean grade:         {df['grade'].mean():.2f}\")\n","\n","    # Split: 70/15/15\n","    train_df, temp_df = train_test_split(\n","        df, test_size=0.3, random_state=42, stratify=df['q']\n","    )\n","    val_df, test_df = train_test_split(\n","        temp_df, test_size=0.5, random_state=42, stratify=temp_df['q']\n","    )\n","\n","    print(f\"\\nğŸ“‚ Data Split:\")\n","    print(f\"   Training:    {len(train_df):>5} ({len(train_df)/len(df)*100:.1f}%)\")\n","    print(f\"   Validation:  {len(val_df):>5} ({len(val_df)/len(df)*100:.1f}%)\")\n","    print(f\"   Test:        {len(test_df):>5} ({len(test_df)/len(df)*100:.1f}%)\")\n","\n","    print(\"=\"*70)\n","\n","    return train_df, val_df, test_df\n","\n","# Process data\n","train_df, val_df, test_df = load_and_split_data(student_data_csv_path) # Use the dedicated CSV path\n","\n","# Save splits to data_processed\n","processed_dir = os.path.join(PROJECT_ROOT, 'data_processed')\n","\n","train_path = os.path.join(processed_dir, 'train_df.pkl')\n","val_path = os.path.join(processed_dir, 'val_df.pkl')\n","test_path = os.path.join(processed_dir, 'test_df.pkl')\n","\n","train_df.to_pickle(train_path)\n","val_df.to_pickle(val_path)\n","test_df.to_pickle(test_path)\n","\n","print(f\"\\nğŸ’¾ Saved splits:\")\n","print(f\"   âœ… {train_path}\")\n","print(f\"   âœ… {val_path}\")\n","print(f\"   âœ… {test_path}\")\n"],"metadata":{"id":"gjUa8NSGUOlw","colab":{"base_uri":"https://localhost:8080/","height":219},"executionInfo":{"status":"error","timestamp":1765509722061,"user_tz":-120,"elapsed":81,"user":{"displayName":"Ahmed Salah","userId":"18015139272935664527"}},"outputId":"12dde678-da47-4acd-bf8d-8bf633401ede"},"execution_count":8,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'student_data_csv_path' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2914198103.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;31m# Process data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_split_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudent_data_csv_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Use the dedicated CSV path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;31m# Save splits to data_processed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'student_data_csv_path' is not defined"]}]},{"cell_type":"code","source":["# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","# CELL 8: Extract Text from Reference PDFs\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","\n","def extract_text_from_pdf(pdf_path):\n","    \"\"\"Extract text from PDF\"\"\"\n","    text = \"\"\n","    try:\n","        with open(pdf_path, 'rb') as file:\n","            pdf_reader = PyPDF2.PdfReader(file)\n","            total_pages = len(pdf_reader.pages)\n","            print(f\"   ğŸ“„ {os.path.basename(pdf_path)}: {total_pages} pages\")\n","\n","            for i, page in enumerate(pdf_reader.pages):\n","                text += page.extract_text() + \"\\n\"\n","\n","                # Progress for large PDFs\n","                if (i + 1) % 50 == 0:\n","                    print(f\"      Processed {i+1}/{total_pages} pages...\")\n","    except Exception as e:\n","        print(f\"   âŒ Error reading {pdf_path}: {e}\")\n","\n","    return text\n","\n","def clean_text(text):\n","    \"\"\"Clean extracted text\"\"\"\n","    # Remove extra whitespace\n","    text = re.sub(r'\\s+', ' ', text)\n","    # Remove common PDF artifacts\n","    text = re.sub(r'Page \\d+', '', text)\n","    text = re.sub(r'\\d+\\s*$', '', text)\n","    # Remove special characters but keep punctuation\n","    text = re.sub(r'[^\\w\\s.,;:?!()\\-]', ' ', text)\n","    return text.strip()\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"ğŸ“– EXTRACTING TEXT FROM REFERENCE PDFs\")\n","print(\"=\"*70)\n","\n","extracted_docs = []\n","\n","for pdf_path in reference_files:\n","    print(f\"\\nğŸ“š Processing: {os.path.basename(pdf_path)}\")\n","    text = extract_text_from_pdf(pdf_path)\n","    cleaned_text = clean_text(text)\n","\n","    extracted_docs.append({\n","        'filename': os.path.basename(pdf_path),\n","        'text': cleaned_text,\n","        'length': len(cleaned_text),\n","        'word_count': len(cleaned_text.split())\n","    })\n","\n","    print(f\"   âœ… Extracted: {len(cleaned_text):,} characters\")\n","    print(f\"   âœ… Words: ~{len(cleaned_text.split()):,}\")\n","\n","print(f\"\\nâœ… Total documents processed: {len(extracted_docs)}\")\n","total_words = sum(doc['word_count'] for doc in extracted_docs)\n","print(f\"âœ… Total words: ~{total_words:,}\")\n","\n","print(\"=\"*70)"],"metadata":{"id":"yC_OlmFMUSdv","executionInfo":{"status":"aborted","timestamp":1765509623886,"user_tz":-120,"elapsed":301859,"user":{"displayName":"Ahmed Salah","userId":"18015139272935664527"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","# CELL 9: Chunk Reference Texts\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","\n","def chunk_text(text, chunk_size=500, overlap=100):\n","    \"\"\"\n","    Split text into overlapping chunks\n","\n","    Args:\n","        text: Input text\n","        chunk_size: Target size in characters (~100 words)\n","        overlap: Overlap between chunks in characters\n","    \"\"\"\n","    words = text.split()\n","    chunks = []\n","\n","    # Approximate words per chunk\n","    words_per_chunk = chunk_size // 5  # ~5 chars per word\n","    overlap_words = overlap // 5\n","\n","    for i in range(0, len(words), words_per_chunk - overlap_words):\n","        chunk_words = words[i:i + words_per_chunk]\n","        chunk = ' '.join(chunk_words)\n","\n","        # Only keep meaningful chunks\n","        if len(chunk.strip()) > 100:  # Minimum 100 chars\n","            chunks.append(chunk.strip())\n","\n","    return chunks\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"âœ‚ï¸  CHUNKING REFERENCE TEXTS\")\n","print(\"=\"*70)\n","print(f\"   Chunk size: ~500 characters (~100 words)\")\n","print(f\"   Overlap: ~100 characters (~20 words)\\n\")\n","\n","all_ref_chunks = []\n","\n","for doc in extracted_docs:\n","    chunks = chunk_text(doc['text'], chunk_size=500, overlap=100)\n","\n","    for i, chunk in enumerate(chunks):\n","        all_ref_chunks.append({\n","            'content': chunk,\n","            'source': doc['filename'],\n","            'chunk_id': i,\n","            'chunk_length': len(chunk)\n","        })\n","\n","    print(f\"   âœ… {doc['filename']}: {len(chunks)} chunks\")\n","\n","print(f\"\\nâœ… Total reference chunks: {len(all_ref_chunks)}\")\n","\n","# Sample chunk\n","if all_ref_chunks:\n","    print(f\"\\nğŸ“‹ Sample chunk:\")\n","    print(f\"   Source: {all_ref_chunks[0]['source']}\")\n","    print(f\"   Length: {all_ref_chunks[0]['chunk_length']} chars\")\n","    print(f\"   Content: {all_ref_chunks[0]['content'][:200]}...\")\n","\n","print(\"=\"*70)\n"],"metadata":{"id":"JDz4C-gtUX_B","executionInfo":{"status":"aborted","timestamp":1765509623893,"user_tz":-120,"elapsed":299809,"user":{"displayName":"Ahmed Salah","userId":"18015139272935664527"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","# CELL 10: Load Embedding Model\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"ğŸ”® LOADING EMBEDDING MODEL\")\n","print(\"=\"*70)\n","\n","print(\"\\nâ³ Loading sentence-transformers...\")\n","print(\"   Model: all-MiniLM-L6-v2\")\n","print(\"   Embedding dimension: 384\\n\")\n","\n","embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n","embedding_model = embedding_model.to('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","print(f\"âœ… Model loaded on: {embedding_model.device}\")\n","\n","# Test\n","test_emb = embedding_model.encode([\"Test sentence\"], convert_to_numpy=True)\n","print(f\"âœ… Test embedding shape: {test_emb.shape}\")\n","\n","print(\"=\"*70)"],"metadata":{"id":"5GS1SzBXUYKf","executionInfo":{"status":"aborted","timestamp":1765509623918,"user_tz":-120,"elapsed":297581,"user":{"displayName":"Ahmed Salah","userId":"18015139272935664527"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","# CELL 11: Create Student Answer Vector Store\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"ğŸ—„ï¸  CREATING STUDENT ANSWER VECTOR STORE\")\n","print(\"=\"*70)\n","\n","print(f\"\\nâ³ Embedding {len(train_df)} training examples...\")\n","print(\"   This will take 5-10 minutes...\\n\")\n","\n","# Prepare data\n","student_answers = train_df['student_answer'].tolist()\n","question_ids = train_df['q'].tolist()\n","grades = train_df['grade'].tolist()\n","model_answers = train_df['model_answer'].tolist()\n","\n","# Create embeddings in batches\n","batch_size = 128\n","student_embeddings = []\n","\n","for i in range(0, len(student_answers), batch_size):\n","    batch = student_answers[i:i+batch_size]\n","    batch_emb = embedding_model.encode(batch, convert_to_numpy=True, show_progress_bar=False)\n","    student_embeddings.append(batch_emb)\n","\n","    processed = min(i + batch_size, len(student_answers))\n","    if processed % 1000 == 0 or processed == len(student_answers):\n","        print(f\"   âœ“ {processed}/{len(student_answers)} ({processed/len(student_answers)*100:.1f}%)\")\n","\n","student_embeddings = np.vstack(student_embeddings)\n","\n","print(f\"\\nâœ… Student embeddings created!\")\n","print(f\"   Shape: {student_embeddings.shape}\")\n","print(f\"   Size: ~{student_embeddings.nbytes / 1e6:.1f} MB\")\n","\n","# Create store\n","student_vector_store = {\n","    'embeddings': student_embeddings,\n","    'student_answers': student_answers,\n","    'question_ids': question_ids,\n","    'grades': grades,\n","    'model_answers': model_answers,\n","    'metadata': {\n","        'total_samples': len(student_answers),\n","        'embedding_dim': student_embeddings.shape[1],\n","        'unique_questions': len(set(question_ids)),\n","        'created_date': datetime.now().isoformat()\n","    }\n","}\n","\n","# Save\n","student_store_path = os.path.join(PROJECT_ROOT, 'features', 'student_vector_store.pkl')\n","with open(student_store_path, 'wb') as f:\n","    pickle.dump(student_vector_store, f)\n","\n","print(f\"\\nğŸ’¾ Saved: {student_store_path}\")\n","print(\"=\"*70)\n"],"metadata":{"id":"7qLEKbnPUsGH","executionInfo":{"status":"aborted","timestamp":1765509623925,"user_tz":-120,"elapsed":295266,"user":{"displayName":"Ahmed Salah","userId":"18015139272935664527"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","# CELL 12: Create Reference Vector Store\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"ğŸ“š CREATING REFERENCE VECTOR STORE\")\n","print(\"=\"*70)\n","\n","print(f\"\\nâ³ Embedding {len(all_ref_chunks)} reference chunks...\")\n","print(\"   This will take 5-15 minutes depending on chunk count...\\n\")\n","\n","# Extract texts\n","ref_texts = [chunk['content'] for chunk in all_ref_chunks]\n","\n","# Create embeddings\n","batch_size = 128\n","ref_embeddings = []\n","\n","for i in range(0, len(ref_texts), batch_size):\n","    batch = ref_texts[i:i+batch_size]\n","    batch_emb = embedding_model.encode(batch, convert_to_numpy=True, show_progress_bar=False)\n","    ref_embeddings.append(batch_emb)\n","\n","    processed = min(i + batch_size, len(ref_texts))\n","    if processed % 500 == 0 or processed == len(ref_texts):\n","        print(f\"   âœ“ {processed}/{len(ref_texts)} ({processed/len(ref_texts)*100:.1f}%)\")\n","\n","ref_embeddings = np.vstack(ref_embeddings)\n","\n","print(f\"\\nâœ… Reference embeddings created!\")\n","print(f\"   Shape: {ref_embeddings.shape}\")\n","print(f\"   Size: ~{ref_embeddings.nbytes / 1e6:.1f} MB\")\n","\n","# Create store\n","reference_vector_store = {\n","    'embeddings': ref_embeddings,\n","    'content': [chunk['content'] for chunk in all_ref_chunks],\n","    'sources': [chunk['source'] for chunk in all_ref_chunks],\n","    'chunk_ids': [chunk['chunk_id'] for chunk in all_ref_chunks],\n","    'metadata': {\n","        'total_chunks': len(all_ref_chunks),\n","        'embedding_dim': ref_embeddings.shape[1],\n","        'source_files': [doc['filename'] for doc in extracted_docs],\n","        'chunk_size': 500,\n","        'overlap': 100,\n","        'created_date': datetime.now().isoformat()\n","    }\n","}\n","\n","# Save\n","ref_store_path = os.path.join(PROJECT_ROOT, 'features', 'reference_vector_store.pkl')\n","with open(ref_store_path, 'wb') as f:\n","    pickle.dump(reference_vector_store, f)\n","\n","print(f\"\\nğŸ’¾ Saved: {ref_store_path}\")\n","print(\"=\"*70)\n"],"metadata":{"id":"z7nbOLf2UhFg","executionInfo":{"status":"aborted","timestamp":1765509623928,"user_tz":-120,"elapsed":293270,"user":{"displayName":"Ahmed Salah","userId":"18015139272935664527"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","# CELL 13: Update Config & Create Summary\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"ğŸ“ UPDATING CONFIG & CREATING SUMMARY\")\n","print(\"=\"*70)\n","\n","# Update config\n","config['created_date'] = datetime.now().isoformat()\n","config['data_info'] = {\n","    'total_samples': len(train_df) + len(val_df) + len(test_df),\n","    'train_samples': len(train_df),\n","    'val_samples': len(val_df),\n","    'test_samples': len(test_df),\n","    'unique_questions': int(train_df['q'].nunique()),\n","    'grade_range': [float(train_df['grade'].min()), float(train_df['grade'].max())],\n","    'reference_chunks': len(all_ref_chunks),\n","    'reference_sources': [doc['filename'] for doc in extracted_docs]\n","}\n","\n","# Save updated config\n","with open(config_path, 'w') as f:\n","    json.dump(config, f, indent=2)\n","\n","print(f\"âœ… Config updated: {config_path}\")\n","\n","# Create summary report\n","summary = {\n","    'preparation_date': datetime.now().isoformat(),\n","    'student_data': {\n","        'total_samples': len(train_df) + len(val_df) + len(test_df),\n","        'train': len(train_df),\n","        'validation': len(val_df),\n","        'test': len(test_df),\n","        'unique_questions': int(train_df['q'].nunique())\n","    },\n","    'reference_data': {\n","        'source_pdfs': len(reference_files),\n","        'total_chunks': len(all_ref_chunks),\n","        'total_words': total_words,\n","        'sources': [os.path.basename(f) for f in reference_files]\n","    },\n","    'vector_stores': {\n","        'student_store': {\n","            'path': student_store_path,\n","            'embeddings': student_embeddings.shape[0],\n","            'size_mb': float(student_embeddings.nbytes / 1e6)\n","        },\n","        'reference_store': {\n","            'path': ref_store_path,\n","            'embeddings': ref_embeddings.shape[0],\n","            'size_mb': float(ref_embeddings.nbytes / 1e6)\n","        }\n","    }\n","}\n","\n","summary_path = os.path.join(PROJECT_ROOT, 'data_preparation_summary.json')\n","with open(summary_path, 'w') as f:\n","    json.dump(summary, f, indent=2)\n","\n","print(f\"âœ… Summary saved: {summary_path}\")\n","\n","print(\"=\"*70)"],"metadata":{"id":"tI42BG4_UYRn","executionInfo":{"status":"aborted","timestamp":1765509623929,"user_tz":-120,"elapsed":290916,"user":{"displayName":"Ahmed Salah","userId":"18015139272935664527"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ffF-kEe_SP9E","executionInfo":{"status":"aborted","timestamp":1765509623931,"user_tz":-120,"elapsed":288230,"user":{"displayName":"Ahmed Salah","userId":"18015139272935664527"}}},"outputs":[],"source":["\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","# CELL 14: Final Verification & Summary\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"âœ… FINAL VERIFICATION\")\n","print(\"=\"*70)\n","\n","# Check all files\n","files_to_verify = {\n","    'Config': config_path,\n","    'Train data': train_path,\n","    'Val data': val_path,\n","    'Test data': test_path,\n","    'Student vectors': student_store_path,\n","    'Reference vectors': ref_store_path,\n","    'Summary': summary_path\n","}\n","\n","print(\"\\nğŸ“ Verifying files in Google Drive:\")\n","all_good = True\n","for name, path in files_to_verify.items():\n","    exists = os.path.exists(path)\n","    size = os.path.getsize(path) / 1e6 if exists else 0\n","    status = \"âœ…\" if exists else \"âŒ\"\n","    print(f\"   {status} {name:<18} ({size:>6.2f} MB)\")\n","    if not exists:\n","        all_good = False\n","\n","if not all_good:\n","    print(\"\\nâš ï¸  Some files missing! Check errors above.\")\n","else:\n","    print(\"\\nğŸ‰ All files verified!\")\n","\n","# Display summary\n","print(\"\\n\" + \"=\"*70)\n","print(\"ğŸ“Š DATA PREPARATION COMPLETE!\")\n","print(\"=\"*70)\n","\n","print(f\"\"\"\n","âœ¨ Successfully processed all data! âœ¨\n","\n","ğŸ“‚ Project Location: {PROJECT_ROOT}\n","\n","ğŸ“Š Student Data:\n","   â€¢ Total samples:     {len(train_df) + len(val_df) + len(test_df):,}\n","   â€¢ Training:          {len(train_df):,}\n","   â€¢ Validation:        {len(val_df):,}\n","   â€¢ Test:              {len(test_df):,}\n","   â€¢ Unique questions:  {train_df['q'].nunique()}\n","\n","ğŸ“š Reference Data:\n","   â€¢ PDF sources:       {len(reference_files)}\n","   â€¢ Total chunks:      {len(all_ref_chunks):,}\n","   â€¢ Total words:       ~{total_words:,}\n","\n","ğŸ’¾ Vector Stores:\n","   â€¢ Student store:     {student_embeddings.shape[0]:,} embeddings ({student_embeddings.nbytes/1e6:.1f} MB)\n","   â€¢ Reference store:   {ref_embeddings.shape[0]:,} embeddings ({ref_embeddings.nbytes/1e6:.1f} MB)\n","\n","ğŸ¯ Next Steps:\n","   1. All files saved to Google Drive âœ…\n","   2. Proceed to Notebook 02: Model Setup\n","   3. Start grading with RAG-enhanced system!\n","\n","ğŸ’¡ Files persist in Google Drive across sessions\n","   No need to re-run this notebook!\n","\"\"\")\n","\n","print(\"=\"*70)\n","\n","# Cleanup\n","del student_embeddings, ref_embeddings\n","gc.collect()\n","torch.cuda.empty_cache()\n","\n","print(\"\\nğŸ§¹ Memory cleaned\")\n","print(\"\\n\" + \"=\"*70)\n","print(\"ğŸ‰ NOTEBOOK 01 COMPLETE!\")\n","print(\"=\"*70)\n","print(\"\\nReady for Notebook 02: Model Setup & Grading System\")"]}]}